# ğŸ§  GenAI Agent Core â€” RAG Pipeline with Local LLM (Mixtral/Mistral)

This project implements a production-ready Retrieval-Augmented Generation (RAG) assistant using **LangChain**, **FAISS**, and a **local LLM (Mistral 7B or Mixtral)**. It supports both CLI and FastAPI interfaces for real-time question answering over embedded documents.

> âœ… Designed for technical interviews, enterprise search, or AI assistant prototyping â€” fully offline-capable and extensible.


âœ… Key Features
	â€¢	ğŸ” RAG architecture: combines retrieval + generation for grounded, explainable answers.
	â€¢	ğŸ¤– Local LLM inference: runs on mistralai/Mistral-7B-Instruct (GGUF via llama.cpp) or HuggingFace models.
	â€¢	ğŸ§  Semantic vector search: uses sentence-transformers and FAISS.
	â€¢	ğŸ“¦ FastAPI backend: exposes /ask and /rebuild endpoints with JSON responses.
	â€¢	ğŸ§° Session memory: persists chat history in PostgreSQL.
	â€¢	ğŸ“ Document ingestion: supports .pdf, .docx, and .txt files via directory-based loader.
	â€¢	ğŸ§ª Swagger docs: self-documenting API.
	â€¢	ğŸ” Offline mode: models can run fully locally with no external API dependencies.

â¸»

ğŸ“ Project Structure

genai-agent-core/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ interface/           # CLI tool for local RAG querying
â”‚   â”‚   â””â”€â”€ query_plus.py    # Full-featured vector+LLM querying
â”‚   â””â”€â”€ llm_core.py          # Basic model call wrappers
â”œâ”€â”€ chat/
â”‚   â”œâ”€â”€ postgres_history.py  # Session-aware chat logging to PostgreSQL
â”‚   â””â”€â”€ vectorstore_memory.py# FAISS vectorstore + ingestion
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ llm_config.yaml      # Model config (OpenAI fallback)
â”œâ”€â”€ data/                    # Place PDFs, DOCX, and TXT files here
â”œâ”€â”€ vectorstore/             # Saved FAISS index
â”œâ”€â”€ models/                  # Local GGUF model storage (Mistral/Mixtral)
â”œâ”€â”€ rag_api_service.py       # FastAPI app with /ask and /rebuild
â”œâ”€â”€ search_vectorstore.py    # CLI tool for searching FAISS chunks
â”œâ”€â”€ rebuild_vs.py            # Manual vectorstore rebuild runner
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Quickstart

### 1. Install dependencies

```bash
conda activate genai-core
pip install -r requirements.txt
pip install sentencepiece
```

---

### 2. Add your documents

Drop your `.pdf`, `.docx`, or `.txt` files into `data/` (or subfolders).

Then run:

```bash
python rebuild_vs.py
```

This rebuilds the FAISS vector index with semantic embeddings using `sentence-transformers`.

---

### 3. Query via CLI

```bash
python app/interface/query_plus.py "What is RAG?" --model mixtral --chat --session-id demo1
```

Options:

* `--model mixtral|llama3|gpt4o`
* `--filter-tag` or `--filter-file` for scoped retrieval
* `--chat` enables persistent memory
* `--session-id` groups chats by user

---

### 4. Query via FastAPI

Start the API:

```bash
uvicorn rag_api_service:app --reload
```

Visit Swagger docs:

```
http://127.0.0.1:8000/docs
```

Example POST to `/ask`:

```json
{
  "query": "What is retrieval-augmented generation?",
  "session_id": "test1",
  "token_estimate": true,
  "model": "llama3"
}
```

Response includes:
	â€¢	answer
	â€¢	sources[]: source text chunks
	â€¢	meta: elapsed_seconds, token count (for OpenAI)

â¸»

ğŸ’¡ Prompt Engineering

Prompts are dynamically constructed using:

* Retrieved chunks (`similarity_search_with_score`)
* Optional session memory
* Custom headers: *"You are an AI document analyst..."*

You can customize this in `generate_prompt()` inside `query_plus.py`.

â¸»

ğŸ§  Model Options
	â€¢	Mixtral/Mistral: supports GGUF via llama.cpp + GPU
	â€¢	LLaMA3 8B: runs via HuggingFace transformers in FP16
	â€¢	GPT-4o: OpenAI fallback with YAML config + API key

To use OpenAI, set:

export OPENAI_API_KEY=sk-...


â¸»

ğŸ—ƒï¸ PostgreSQL Chat History

Chat logs are stored in chathist.chat_history:
	â€¢	session_id, role, content, created_at
	â€¢	Used for session memory and context carryover

Uses .pgpass for secure auth (no credentials in code).

â¸»

ğŸ“Š Metadata & Monitoring

API responses include:
	â€¢	elapsed_seconds: total inference + retrieval time
	â€¢	tokens: OpenAI usage count if applicable

Future additions:
	â€¢	Token estimation for local models
	â€¢	Latency breakdowns
	â€¢	Prometheus metrics

â¸»

ğŸ§ª Testing / Debugging Tools

Tool	Command
Search vectorstore	python search_vectorstore.py "your query"
Rebuild index	python rebuild_vs.py
API call	curl -X POST http://127.0.0.1:8000/ask ...


â¸»

ğŸ” Offline-First Capabilities

Set local_files_only=True in AutoTokenizer and AutoModelForCausalLM to enforce offline mode.

â¸»

ğŸŒ Future Roadmap
	â€¢	âœ… Swagger-enabled API
	â€¢	âœ… Chat memory with PostgreSQL
	â€¢	â³ User auth or API key guardrails
	â€¢	â³ Vectorstore auto-refresh with S3 uploads
	â€¢	â³ Web frontend (Streamlit or React)
	â€¢	â³ GCP Cloud Run deployment config

â¸»